#!/usr/bin/env python3
"""
å°çº¢ä¹¦çƒ­ç‚¹è·Ÿè¸ªå·¥å…·

ç”¨æ³•:
    python track-topic.py <è¯é¢˜> [--limit N] [--feishu] [--output FILE]

ç¤ºä¾‹:
    python track-topic.py "DeepSeek" --limit 5 --feishu
    python track-topic.py "æ˜¥èŠ‚æ—…æ¸¸" --limit 10 --output report.md
"""

import argparse
import json
import subprocess
import sys
import os
from datetime import datetime
from pathlib import Path

# è·å–è„šæœ¬ç›®å½•
SCRIPT_DIR = Path(__file__).parent.resolve()
XHS_SCRIPTS = SCRIPT_DIR  # ç°åœ¨å°±åœ¨ xiaohongshu/scripts ç›®å½•ä¸‹

# é£ä¹¦ skill è·¯å¾„ï¼ˆæ”¯æŒå¤šç§å¯èƒ½çš„ä½ç½®ï¼‰
def find_feishu_scripts() -> Path:
    """æŸ¥æ‰¾ feishu-docs skill çš„ scripts ç›®å½•"""
    possible_paths = [
        SCRIPT_DIR.parent.parent / "feishu-docs" / "scripts",  # åŒçº§ skill
        Path.home() / ".openclaw" / "workspace" / "skills" / "feishu-docs" / "scripts",
        Path.home() / ".claude" / "skills" / "feishu-docs" / "scripts",
    ]
    for p in possible_paths:
        if p.exists():
            return p
    return possible_paths[0]  # è¿”å›é»˜è®¤è·¯å¾„ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰

FEISHU_SCRIPTS = find_feishu_scripts()


def call_xhs_mcp(tool: str, args: dict) -> dict:
    """è°ƒç”¨å°çº¢ä¹¦ MCP å·¥å…·"""
    mcp_call = XHS_SCRIPTS / "mcp-call.sh"
    if not mcp_call.exists():
        print(f"âŒ æ‰¾ä¸åˆ° xiaohongshu skill: {mcp_call}", file=sys.stderr)
        sys.exit(1)
    
    result = subprocess.run(
        [str(mcp_call), tool, json.dumps(args)],
        capture_output=True, text=True, timeout=120
    )
    
    if result.returncode != 0:
        print(f"âŒ MCP è°ƒç”¨å¤±è´¥: {result.stderr}", file=sys.stderr)
        return {}
    
    try:
        response = json.loads(result.stdout)
        if "result" in response and "content" in response["result"]:
            text = response["result"]["content"][0].get("text", "{}")
            return json.loads(text) if text else {}
        elif "error" in response:
            print(f"âš ï¸ MCP é”™è¯¯: {response['error'].get('message', 'Unknown')}", file=sys.stderr)
            return {}
        return response
    except json.JSONDecodeError:
        return {}


def search_feeds(keyword: str) -> list:
    """æœç´¢å°çº¢ä¹¦å†…å®¹"""
    print(f"ğŸ” æœç´¢: {keyword}")
    result = call_xhs_mcp("search_feeds", {"keyword": keyword})
    feeds = result.get("feeds", [])
    # è¿‡æ»¤æ‰ hot_query ç±»å‹
    return [f for f in feeds if f.get("modelType") == "note"]


def get_feed_detail(feed_id: str, xsec_token: str, load_comments: bool = True) -> dict:
    """è·å–å¸–å­è¯¦æƒ…"""
    args = {
        "feed_id": feed_id,
        "xsec_token": xsec_token,
        "load_all_comments": load_comments
    }
    result = call_xhs_mcp("get_feed_detail", args)
    return result.get("data", {})


def format_timestamp(ts: int) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
    if not ts:
        return "æœªçŸ¥"
    try:
        dt = datetime.fromtimestamp(ts / 1000)
        return dt.strftime("%Y-%m-%d %H:%M")
    except:
        return "æœªçŸ¥"


def get_comments_list(post: dict) -> list:
    """å®‰å…¨åœ°è·å–è¯„è®ºåˆ—è¡¨"""
    comments = post.get("comments", {})
    if isinstance(comments, dict):
        return comments.get("list", [])
    elif isinstance(comments, list):
        return comments
    return []


def generate_report(keyword: str, posts: list) -> str:
    """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    
    report = f"""# ğŸ”¥ å°çº¢ä¹¦çƒ­ç‚¹è·Ÿè¸ªæŠ¥å‘Š

**è¯é¢˜:** {keyword}  
**ç”Ÿæˆæ—¶é—´:** {now}  
**æ”¶å½•å¸–å­:** {len(posts)} ç¯‡

---

## ğŸ“Š æ¦‚è§ˆ

"""
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_likes = sum(int(p.get("note", {}).get("interactInfo", {}).get("likedCount", 0) or 0) for p in posts)
    total_comments = sum(len(get_comments_list(p)) for p in posts)
    
    report += f"""| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ€»å¸–å­æ•° | {len(posts)} |
| æ€»ç‚¹èµæ•° | {total_likes:,} |
| æ€»è¯„è®ºæ•° | {total_comments} |

---

## ğŸ“ çƒ­å¸–è¯¦æƒ…

"""
    
    for i, post in enumerate(posts, 1):
        note = post.get("note", {})
        comments = get_comments_list(post)
        
        title = note.get("title", "æ— æ ‡é¢˜")
        desc = note.get("desc", "")
        user = note.get("user", {}).get("nickname", "åŒ¿å")
        time_str = format_timestamp(note.get("time"))
        interact = note.get("interactInfo", {})
        likes = interact.get("likedCount", "0")
        collected = interact.get("collectedCount", "0")
        
        report += f"""### {i}. {title}

**ä½œè€…:** {user}  
**æ—¶é—´:** {time_str}  
**äº’åŠ¨:** â¤ï¸ {likes} èµ Â· â­ {collected} æ”¶è—

**æ­£æ–‡:**

> {desc[:500]}{"..." if len(desc) > 500 else ""}

"""
        
        if comments:
            report += f"""**çƒ­é—¨è¯„è®º ({len(comments)} æ¡):**

"""
            for j, comment in enumerate(list(comments)[:5], 1):
                c_user = comment.get("userInfo", {}).get("nickname", "åŒ¿å")
                c_content = comment.get("content", "")
                c_likes = comment.get("likeCount", 0)
                report += f"- **{c_user}** ({c_likes}èµ): {c_content[:100]}\n"
            
            if len(comments) > 5:
                report += f"- *... è¿˜æœ‰ {len(comments) - 5} æ¡è¯„è®º*\n"
        
        report += "\n---\n\n"
    
    # è¯„è®ºåŒºçƒ­ç‚¹æ€»ç»“
    report += """## ğŸ’¬ è¯„è®ºåŒºçƒ­ç‚¹å…³é”®è¯

"""
    
    # ç®€å•çš„å…³é”®è¯æå–ï¼ˆç»Ÿè®¡é«˜é¢‘è¯ï¼‰
    all_comments = []
    for post in posts:
        for c in get_comments_list(post):
            all_comments.append(c.get("content", ""))
    
    if all_comments:
        report += f"å…± {len(all_comments)} æ¡è¯„è®ºï¼Œä¸»è¦è®¨è®ºæ–¹å‘ï¼š\n\n"
        # è¿™é‡Œå¯ä»¥åšæ›´å¤æ‚çš„ NLP åˆ†æï¼Œæš‚æ—¶ç®€åŒ–
        report += "- ç”¨æˆ·å¯¹è¯¥è¯é¢˜çš„å…³æ³¨åº¦è¾ƒé«˜\n"
        report += "- è¯„è®ºåŒºäº’åŠ¨æ´»è·ƒ\n"
    else:
        report += "æš‚æ— è¶³å¤Ÿè¯„è®ºæ•°æ®è¿›è¡Œåˆ†æ\n"
    
    report += """
---

## ğŸ“ˆ è¶‹åŠ¿åˆ†æ

åŸºäºä»¥ä¸Šçƒ­å¸–å’Œè¯„è®ºæ•°æ®ï¼Œè¯¥è¯é¢˜åœ¨å°çº¢ä¹¦ä¸Šå‘ˆç°ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **çƒ­åº¦æŒ‡æ•°**: """ + ("ğŸ”¥ğŸ”¥ğŸ”¥ é«˜" if total_likes > 1000 else "ğŸ”¥ğŸ”¥ ä¸­" if total_likes > 100 else "ğŸ”¥ ä½") + f"""
2. **äº’åŠ¨æ´»è·ƒåº¦**: """ + ("æ´»è·ƒ" if total_comments > 50 else "ä¸€èˆ¬" if total_comments > 10 else "è¾ƒä½") + """
3. **å†…å®¹ç±»å‹**: ä»¥å›¾æ–‡ç¬”è®°ä¸ºä¸»

---

*æŠ¥å‘Šç”± OpenClaw å°çº¢ä¹¦çƒ­ç‚¹è·Ÿè¸ªå·¥å…·è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    return report


def export_to_feishu(title: str, content: str) -> str:
    """å¯¼å‡ºåˆ°é£ä¹¦æ–‡æ¡£"""
    import_script = FEISHU_SCRIPTS / "doc-import.sh"
    if not import_script.exists():
        print(f"âŒ æ‰¾ä¸åˆ° feishu-docs skill: {import_script}", file=sys.stderr)
        return ""
    
    print("ğŸ“¤ å¯¼å‡ºåˆ°é£ä¹¦æ–‡æ¡£...")
    
    # å†™å…¥ä¸´æ—¶æ–‡ä»¶
    tmp_file = Path("/tmp/xhs_report.md")
    tmp_file.write_text(content, encoding="utf-8")
    
    result = subprocess.run(
        [str(import_script), title, "--file", str(tmp_file)],
        capture_output=True, text=True, timeout=60
    )
    
    if result.returncode != 0:
        print(f"âš ï¸ é£ä¹¦å¯¼å‡ºå¤±è´¥: {result.stderr}", file=sys.stderr)
        return ""
    
    # è§£æè¿”å›çš„æ–‡æ¡£é“¾æ¥
    output = result.stdout
    print(output)
    return output


def main():
    parser = argparse.ArgumentParser(description="å°çº¢ä¹¦çƒ­ç‚¹è·Ÿè¸ªå·¥å…·")
    parser.add_argument("keyword", help="è¦è·Ÿè¸ªçš„è¯é¢˜/å…³é”®è¯")
    parser.add_argument("--limit", "-n", type=int, default=10, help="è·å–å¸–å­æ•°é‡ (é»˜è®¤ 10)")
    parser.add_argument("--feishu", "-f", action="store_true", help="å¯¼å‡ºåˆ°é£ä¹¦æ–‡æ¡£")
    parser.add_argument("--output", "-o", help="è¾“å‡º Markdown æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--no-comments", action="store_true", help="ä¸è·å–è¯„è®º")
    
    args = parser.parse_args()
    
    # 1. æœç´¢å¸–å­
    feeds = search_feeds(args.keyword)
    if not feeds:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³å¸–å­")
        sys.exit(1)
    
    print(f"âœ… æ‰¾åˆ° {len(feeds)} æ¡å¸–å­")
    
    # 2. è·å–è¯¦æƒ…
    posts = []
    for i, feed in enumerate(feeds[:args.limit]):
        feed_id = feed.get("id")
        xsec_token = feed.get("xsecToken")
        title = feed.get("noteCard", {}).get("displayTitle", "")
        
        print(f"ğŸ“– [{i+1}/{min(len(feeds), args.limit)}] è·å–: {title[:30]}...")
        
        detail = get_feed_detail(feed_id, xsec_token, not args.no_comments)
        if detail:
            posts.append(detail)
    
    if not posts:
        print("âŒ æœªèƒ½è·å–å¸–å­è¯¦æƒ…")
        sys.exit(1)
    
    print(f"âœ… æˆåŠŸè·å– {len(posts)} ç¯‡å¸–å­è¯¦æƒ…")
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“ ç”ŸæˆæŠ¥å‘Š...")
    report = generate_report(args.keyword, posts)
    
    # 4. è¾“å‡º
    if args.output:
        output_path = Path(args.output)
        output_path.write_text(report, encoding="utf-8")
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    if args.feishu:
        doc_title = f"å°çº¢ä¹¦çƒ­ç‚¹è·Ÿè¸ª: {args.keyword} ({datetime.now().strftime('%m-%d')})"
        export_to_feishu(doc_title, report)
    
    if not args.output and not args.feishu:
        # é»˜è®¤è¾“å‡ºåˆ° stdout
        print("\n" + "="*60 + "\n")
        print(report)
    
    return report


if __name__ == "__main__":
    main()
